#!/usr/bin/env python3
"""
ü¶ñ Enhanced AI System for Restaceratops
Advanced AI system with multi-model support, RAG integration, and context-aware responses
"""

import os
import json
import logging
from typing import List, Dict, Optional, Any, Tuple
from datetime import datetime
import asyncio

from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage, AIMessage, SystemMessage
from langchain.memory import ConversationBufferWindowMemory

# Import free AI model support (OpenRouter handled separately)
OLLAMA_AVAILABLE = False
HUGGINGFACE_AVAILABLE = False

# Import Gemini AI support
try:
    from langchain_google_genai import ChatGoogleGenerativeAI
    GEMINI_AVAILABLE = True
except ImportError:
    GEMINI_AVAILABLE = False

from backend.core.models.vector_store import get_vector_store, setup_vector_store
from backend.core.agents.rag_system import get_rag_system

log = logging.getLogger("agent.enhanced_ai_system")

class MultiProviderAI:
    """Manages multiple AI providers with automatic fallback."""
    
    def __init__(self):
        self.providers = []
        self.current_provider_index = 0
        self.failed_providers = set()
        
        # Initialize all available providers
        self._initialize_providers()
    
    def _initialize_providers(self):
        """Initialize AI providers: Gemini (Primary) and OpenRouter (Backup) - Free models only."""
        
        # 1. Gemini (Primary - Google AI - Free tier)
        if GEMINI_AVAILABLE:
            gemini_key = os.getenv("GOOGLE_API_KEY")
            if gemini_key and gemini_key != "your-google-api-key-here":
                self.providers.append({
                    "name": "Gemini-AI",
                    "type": "gemini",
                    "api_key": gemini_key,
                    "model": "gemini-1.5-flash",  # Free model: 15 req/min
                    "priority": 1
                })
                log.info("‚úÖ Gemini AI configured as primary provider (Free tier)")
        
        # 2. OpenRouter (Backup - Multiple free models)
        openrouter_key = os.getenv("OPENROUTER_API_KEY")
        if openrouter_key and openrouter_key != "your-openrouter-api-key-here":
            self.providers.append({
                "name": "OpenRouter-AI",
                "type": "openrouter",
                "api_key": openrouter_key,
                "model": "openai/gpt-3.5-turbo",  # Free model
                "priority": 2
            })
            log.info("‚úÖ OpenRouter AI configured as backup provider (Free models)")
        
        # Sort by priority
        self.providers.sort(key=lambda x: x["priority"])
        
        if not self.providers:
            log.warning("‚ö†Ô∏è No AI providers configured - system will use fallback responses")
        else:
            log.info(f"üéØ Initialized {len(self.providers)} AI providers (Free models only)")
            for provider in self.providers:
                log.info(f"  - {provider['name']} ({provider['type']}) - Priority {provider['priority']}")
    
    def get_next_working_provider(self):
        """Get the next working provider, skipping failed ones."""
        attempts = 0
        while attempts < len(self.providers):
            provider = self.providers[self.current_provider_index]
            
            if provider["name"] not in self.failed_providers:
                return provider
            
            # Move to next provider
            self.current_provider_index = (self.current_provider_index + 1) % len(self.providers)
            attempts += 1
        
        # If all providers failed, reset and try again
        self.failed_providers.clear()
        self.current_provider_index = 0
        return self.providers[0] if self.providers else None
    
    def mark_provider_failed(self, provider_name: str):
        """Mark a provider as failed."""
        self.failed_providers.add(provider_name)
        log.warning(f"Marked provider {provider_name} as failed")
    
    def create_llm(self, provider: Dict) -> Optional[Any]:
        """Create an LLM instance for the given provider (Gemini or OpenRouter only)."""
        try:
            if provider["type"] == "gemini":
                if GEMINI_AVAILABLE:
                    return ChatGoogleGenerativeAI(
                        model=provider.get("model", "gemini-1.5-flash"),
                        google_api_key=provider["api_key"],
                        temperature=0.7,
                        max_output_tokens=4000,
                        convert_system_message_to_human=True
                    )
                else:
                    log.error("Gemini AI not available - missing dependencies")
                    return None
            
            elif provider["type"] == "openrouter":
                # For OpenRouter, we'll use the OpenRouterAISystem directly
                # This ensures we get the free model fallback logic
                from backend.core.agents.openrouter_ai_system import get_openrouter_ai_system
                openrouter_system = get_openrouter_ai_system()
                if openrouter_system.api_key:
                    return openrouter_system
                else:
                    log.error("OpenRouter API key not configured")
                    return None
            
        except Exception as e:
            log.error(f"Failed to create LLM for {provider['name']}: {e}")
            return None
        
        return None

class EnhancedAISystem:
    """Advanced AI system for Restaceratops with multi-model support and RAG integration."""
    
    def __init__(self, api_key: Optional[str] = None, model: str = "gpt-4o-mini", 
                 use_free_models: bool = True, free_model_provider: str = "auto"):
        """Initialize the enhanced AI system."""
        self.api_key = api_key or os.getenv("OPENAI_API_KEY")
        self.model = model
        self.use_free_models = use_free_models
        self.free_model_provider = free_model_provider
        
        # Initialize multi-provider AI system
        self.multi_provider = MultiProviderAI()
        
        # Initialize AI models
        self.models = {
            'conversation': 'gpt-4o-mini',
            'test_generation': 'gpt-4o',
            'code_analysis': 'gpt-4o',
            'troubleshooting': 'gpt-4o-mini'
        }
        
        # Initialize LLM instances
        self.llms = {}
        
        # Initialize vector store
        self.vector_store = get_vector_store()
        
        # Initialize RAG system
        self.rag_system = get_rag_system(api_key, self.models['conversation'])
        
        # Initialize conversation memory (using new LangChain memory system)
        try:
            from langchain_core.memory import ConversationBufferWindowMemory
            self.conversation_memory = ConversationBufferWindowMemory(
                k=10,  # Remember last 10 exchanges
                return_messages=True
            )
        except ImportError:
            # Fallback to old import if new one not available
            from langchain.memory import ConversationBufferWindowMemory
            self.conversation_memory = ConversationBufferWindowMemory(
                k=10,  # Remember last 10 exchanges
                return_messages=True
            )
        
        # System prompts for different tasks
        self.system_prompts = {
            'conversation': """You are Restaceratops, an AI-powered API testing agent with advanced capabilities.

Your personality:
- Friendly and helpful dinosaur-themed assistant
- Expert in API testing and automation
- Patient and thorough in explanations
- Always provide actionable advice

Your capabilities:
- Test APIs and endpoints automatically
- Generate comprehensive test cases
- Analyze API specifications
- Provide troubleshooting guidance
- Create detailed reports
- Explain complex concepts simply

Always maintain your dinosaur theme with ü¶ñ emojis and be helpful!""",

            'test_generation': """You are an expert API testing specialist. Generate comprehensive, production-ready test cases.

Requirements:
- Use YAML format for test definitions
- Include positive and negative test scenarios
- Test authentication, error handling, and edge cases
- Use realistic test data
- Follow API testing best practices
- Include proper assertions and validations

Generate tests that are:
- Comprehensive and thorough
- Easy to understand and maintain
- Production-ready and reliable
- Well-documented with clear descriptions""",

            'troubleshooting': """You are an expert API testing troubleshooter. Provide detailed, actionable solutions.

Your approach:
- Identify the root cause of issues
- Provide step-by-step solutions
- Suggest preventive measures
- Use clear, technical language
- Include code examples when helpful
- Consider security implications

Always be thorough and helpful in your troubleshooting guidance.""",

            'code_analysis': """You are an expert API code analyst. Analyze code and provide insights.

Your analysis should include:
- Code quality assessment
- Security considerations
- Performance implications
- Best practices recommendations
- Potential improvements
- Testing strategy suggestions

Provide clear, actionable feedback with examples."""
        }
        
        log.info(f"Enhanced AI system initialized with {len(self.multi_provider.providers)} providers")
    
    async def _get_working_llm(self, task_type: str = 'conversation') -> Optional[Any]:
        """Get a working LLM instance with automatic fallback."""
        max_attempts = len(self.multi_provider.providers)
        attempts = 0
        
        while attempts < max_attempts:
            provider = self.multi_provider.get_next_working_provider()
            if not provider:
                break
            
            try:
                log.info(f"Trying provider: {provider['name']}")
                llm = self.multi_provider.create_llm(provider)
                
                if llm:
                    # Test the LLM with a simple request
                    test_message = "Hello"
                    response = await llm.agenerate([[HumanMessage(content=test_message)]])
                    
                    if response and response.generations:
                        log.info(f"Successfully connected to {provider['name']}")
                        return llm
                
            except Exception as e:
                log.warning(f"Provider {provider['name']} failed: {e}")
                self.multi_provider.mark_provider_failed(provider['name'])
            
            attempts += 1
        
        log.error("All AI providers failed")
        return None
    
    async def handle_conversation(self, user_input: str) -> str:
        """Handle user conversation with automatic provider fallback."""
        try:
            llm = await self._get_working_llm('conversation')
            
            if llm:
                # Get conversation history
                messages = self.conversation_memory.load_memory_variables({})
                history = messages.get("history", [])
                
                # Add system prompt
                all_messages = [SystemMessage(content=self.system_prompts['conversation'])]
                all_messages.extend(history)
                all_messages.append(HumanMessage(content=user_input))
                
                # Generate response
                response = await llm.agenerate([all_messages])
                
                if response and response.generations:
                    ai_response = response.generations[0][0].text
                    
                    # Update memory
                    self.conversation_memory.save_context(
                        {"input": user_input},
                        {"output": ai_response}
                    )
                    
                    return ai_response
            
            # Fallback to enhanced responses
            return self._get_fallback_response(user_input)
            
        except Exception as e:
            log.error(f"Error in conversation: {e}")
            return self._get_fallback_response(user_input)
    
    async def generate_intelligent_tests(self, api_spec: str, requirements: str) -> str:
        """Generate intelligent test cases with automatic provider fallback."""
        try:
            llm = await self._get_working_llm('test_generation')
            
            if llm:
                prompt = f"""Generate comprehensive API test cases for: {api_spec}

Requirements: {requirements}

Generate YAML test cases that include:
- Health checks
- Authentication tests
- Error handling
- Edge cases
- Realistic test data

Return only valid YAML, no markdown formatting."""

                response = await llm.agenerate([[HumanMessage(content=prompt)]])
                
                if response and response.generations:
                    return response.generations[0][0].text
            
            # Fallback to RAG system
            return self.rag_system._get_fallback_test_generation(api_spec, requirements)
            
        except Exception as e:
            log.error(f"Error generating tests: {e}")
            return self.rag_system._get_fallback_test_generation(api_spec, requirements)
    
    async def generate_troubleshooting_guide(self, error_description: str) -> str:
        """Generate troubleshooting guide with automatic provider fallback."""
        try:
            llm = await self._get_working_llm('troubleshooting')
            
            if llm:
                prompt = f"""Provide a detailed troubleshooting guide for this API testing issue:

{error_description}

Include:
- Root cause analysis
- Step-by-step solutions
- Prevention strategies
- Code examples if relevant

Be thorough and actionable."""

                response = await llm.agenerate([[HumanMessage(content=prompt)]])
                
                if response and response.generations:
                    return response.generations[0][0].text
            
            # Fallback to RAG system
            return self.rag_system._get_fallback_troubleshooting(error_description)
            
        except Exception as e:
            log.error(f"Error generating troubleshooting: {e}")
            return self.rag_system._get_fallback_troubleshooting(error_description)
    
    def get_system_stats(self) -> Dict[str, Any]:
        """Get system statistics including provider status."""
        stats = {
            "total_providers": len(self.multi_provider.providers),
            "working_providers": len(self.multi_provider.providers) - len(self.multi_provider.failed_providers),
            "failed_providers": list(self.multi_provider.failed_providers),
            "current_provider": self.multi_provider.providers[self.multi_provider.current_provider_index]["name"] if self.multi_provider.providers else "None",
            "providers": [
                {
                    "name": p["name"],
                    "type": p["type"],
                    "status": "working" if p["name"] not in self.multi_provider.failed_providers else "failed"
                }
                for p in self.multi_provider.providers
            ]
        }
        return stats
    
    def _get_fallback_response(self, user_input: str) -> str:
        """Enhanced fallback response when AI models are not available."""
        user_input_lower = user_input.lower().strip()
        
        # Enhanced keyword-based responses
        if any(word in user_input_lower for word in ['hello', 'hi', 'hey', 'greetings']):
            return """ü¶ñ Hello! I'm Restaceratops, your AI-powered API testing agent!

I can help you with:
‚Ä¢ üß™ **API Testing**: Run comprehensive tests on your APIs
‚Ä¢ üìù **Test Generation**: Create test cases from OpenAPI specs
‚Ä¢ üîç **Troubleshooting**: Debug API issues and errors
‚Ä¢ üìä **Reporting**: Generate detailed test reports
‚Ä¢ üõ†Ô∏è **Automation**: Set up CI/CD testing pipelines

**Quick Start:**
‚Ä¢ "Test my API at https://httpbin.org"
‚Ä¢ "Create tests for authentication"
‚Ä¢ "Show me test results"
‚Ä¢ "Help me debug a 500 error"

What would you like to work on today?"""

        elif any(word in user_input_lower for word in ['test', 'run', 'execute', 'check']):
            if any(word in user_input_lower for word in ['api', 'endpoint', 'url']):
                return """üß™ **API Testing Ready!**

I can help you test your APIs! Here's what I can do:

**Basic Testing:**
‚Ä¢ Health checks and status endpoints
‚Ä¢ Authentication flows
‚Ä¢ CRUD operations (GET, POST, PUT, DELETE)
‚Ä¢ Error handling and edge cases
‚Ä¢ Performance and response time validation

**Advanced Features:**
‚Ä¢ Schema validation
‚Ä¢ Variable capture and reuse
‚Ä¢ Concurrent test execution
‚Ä¢ Custom assertions
‚Ä¢ Detailed reporting

**To get started:**
1. Tell me your API URL: "Test my API at https://your-api.com"
2. I'll create and run comprehensive tests
3. You'll get detailed results and recommendations

**Example commands:**
‚Ä¢ "Test my API at https://httpbin.org"
‚Ä¢ "Create tests for user authentication"
‚Ä¢ "Run performance tests on my API"

Would you like me to test a specific API?"""

        elif any(word in user_input_lower for word in ['create', 'generate', 'make', 'write']):
            if any(word in user_input_lower for word in ['test', 'case']):
                return """üìù **Test Generation Ready!**

I can create comprehensive test cases for your APIs! Here's what I generate:

**Test Types:**
‚Ä¢ ‚úÖ **Functional Tests**: Verify endpoints work correctly
‚Ä¢ üîê **Authentication Tests**: Test login, tokens, permissions
‚Ä¢ üõ°Ô∏è **Security Tests**: Validate input, check vulnerabilities
‚Ä¢ ‚ö° **Performance Tests**: Measure response times
‚Ä¢ üîÑ **Integration Tests**: Test data flow between endpoints

**Generation Methods:**
‚Ä¢ **From OpenAPI/Swagger specs** (recommended)
‚Ä¢ **From API URLs** (basic tests)
‚Ä¢ **From your descriptions** (custom scenarios)

**To generate tests:**
1. "Create tests from https://api.example.com/swagger.json"
2. "Generate tests for user management endpoints"
3. "Make authentication test cases"

**Example:**
‚Ä¢ "Create tests from my OpenAPI spec"
‚Ä¢ "Generate tests for user CRUD operations"
‚Ä¢ "Make tests for OAuth2 authentication"

What type of tests would you like me to create?"""

        elif any(word in user_input_lower for word in ['help', 'what', 'how', 'explain']):
            return """ü¶ñ **Restaceratops Help Guide**

I'm your AI-powered API testing assistant! Here's how I can help:

**üéØ Core Capabilities:**
‚Ä¢ **API Testing**: Run comprehensive tests on any API
‚Ä¢ **Test Generation**: Create tests from OpenAPI specs
‚Ä¢ **Troubleshooting**: Debug API issues and errors
‚Ä¢ **Reporting**: Generate detailed test reports
‚Ä¢ **Automation**: Set up CI/CD testing pipelines

**üöÄ Quick Start Commands:**
‚Ä¢ "Test my API at https://your-api.com"
‚Ä¢ "Create tests from my OpenAPI spec"
‚Ä¢ "Show me test results"
‚Ä¢ "Help me debug a 500 error"

**üìö Common Use Cases:**
1. **New API Testing**: "Test my new API endpoints"
2. **Authentication**: "Create tests for login/logout"
3. **Performance**: "Run performance tests on my API"
4. **Debugging**: "Help me fix this API error"
5. **Documentation**: "Generate test documentation"

**üõ†Ô∏è Advanced Features:**
‚Ä¢ Schema validation
‚Ä¢ Variable capture and reuse
‚Ä¢ Concurrent test execution
‚Ä¢ Custom assertions
‚Ä¢ Multiple report formats (HTML, JUnit, Prometheus)

**Need specific help?**
‚Ä¢ "How do I test authentication?"
‚Ä¢ "Explain JSON schema validation"
‚Ä¢ "What are API testing best practices?"

What would you like to learn more about?"""

        elif any(word in user_input_lower for word in ['error', 'debug', 'fix', 'troubleshoot']):
            return """üîç **API Troubleshooting Assistant**

I can help you debug API issues! Here's what I can do:

**Common Problems I Can Help With:**
‚Ä¢ üö´ **4xx Errors**: Bad requests, authentication issues
‚Ä¢ ‚ö†Ô∏è **5xx Errors**: Server errors, timeouts
‚Ä¢ üîê **Authentication**: Token issues, permission problems
‚Ä¢ üìä **Performance**: Slow responses, timeouts
‚Ä¢ üîÑ **Data Issues**: Invalid responses, schema mismatches

**Debugging Process:**
1. **Identify the Problem**: What error are you seeing?
2. **Analyze the Context**: When does it happen?
3. **Test Hypotheses**: Run targeted tests
4. **Provide Solutions**: Fix recommendations

**To get help:**
‚Ä¢ "Help me debug a 500 error"
‚Ä¢ "My API is returning 401 errors"
‚Ä¢ "Authentication is failing"
‚Ä¢ "Response times are too slow"

**Example debugging session:**
‚Ä¢ "I'm getting 500 errors on POST requests"
‚Ä¢ "My API returns 401 for valid tokens"
‚Ä¢ "Response times are over 5 seconds"

What specific issue are you experiencing?"""

        elif any(word in user_input_lower for word in ['report', 'results', 'status', 'show']):
            return """üìä **Test Results & Reporting**

I can provide comprehensive test reports! Here's what I track:

**üìà Metrics I Monitor:**
‚Ä¢ **Success Rate**: Percentage of passing tests
‚Ä¢ **Response Times**: Average, min, max latency
‚Ä¢ **Error Rates**: Types and frequency of failures
‚Ä¢ **Coverage**: Endpoints and scenarios tested
‚Ä¢ **Performance**: Throughput and load handling

**üìã Report Types:**
‚Ä¢ **Console Reports**: Real-time test progress
‚Ä¢ **HTML Reports**: Beautiful, interactive dashboards
‚Ä¢ **JUnit XML**: CI/CD integration format
‚Ä¢ **Prometheus**: Metrics for monitoring systems

**üìä What You'll See:**
‚Ä¢ Test execution progress
‚Ä¢ Pass/fail status for each test
‚Ä¢ Detailed error messages
‚Ä¢ Performance metrics
‚Ä¢ Recommendations for improvement

**To view results:**
‚Ä¢ "Show me the test results"
‚Ä¢ "Generate a test report"
‚Ä¢ "What's the status of my tests?"
‚Ä¢ "Create an HTML report"

**Example reports:**
‚Ä¢ "Tests completed: 15/15 passed"
‚Ä¢ "Average response time: 245ms"
‚Ä¢ "2 authentication tests failed"

Would you like me to run tests and show you the results?"""

        else:
            return """ü¶ñ **Restaceratops AI Assistant**

I'm here to help with API testing and automation! Here are some things I can do:

**üß™ Testing:**
‚Ä¢ Run comprehensive API tests
‚Ä¢ Generate test cases from OpenAPI specs
‚Ä¢ Validate responses and schemas
‚Ä¢ Test authentication and security

**üîß Development:**
‚Ä¢ Debug API issues
‚Ä¢ Optimize performance
‚Ä¢ Set up CI/CD pipelines
‚Ä¢ Create test documentation

**üìä Analysis:**
‚Ä¢ Generate detailed reports
‚Ä¢ Analyze API performance
‚Ä¢ Identify potential issues
‚Ä¢ Provide improvement recommendations

**Quick Commands:**
‚Ä¢ "Test my API at https://your-api.com"
‚Ä¢ "Create tests for authentication"
‚Ä¢ "Help me debug this error"
‚Ä¢ "Show me test results"

What would you like to work on? I'm here to help make your API testing easier and more effective! üöÄ"""

        return self.rag_system._get_fallback_response(user_input)
    
    def _get_fallback_api_analysis(self, api_spec: str) -> str:
        """Fallback API analysis when AI models are not available."""
        return f"""ü¶ñ API Specification Analysis

**API Specification:**
{api_spec[:200]}...

**Basic Analysis:**
‚Ä¢ Review the API endpoints and methods
‚Ä¢ Check authentication requirements
‚Ä¢ Validate request/response formats
‚Ä¢ Test error handling scenarios
‚Ä¢ Verify documentation completeness

**Recommended Testing:**
‚Ä¢ Create tests for each endpoint
‚Ä¢ Test authentication flows
‚Ä¢ Validate response schemas
‚Ä¢ Test error conditions
‚Ä¢ Performance testing

**Next Steps:**
‚Ä¢ Generate comprehensive test cases
‚Ä¢ Set up automated testing
‚Ä¢ Monitor API performance
‚Ä¢ Document test results"""
    

    
    async def reset_system(self) -> str:
        """Reset the enhanced AI system."""
        try:
            # Reset conversation memory
            self.conversation_memory.clear()
            
            # Reset vector store collections
            await self.vector_store.reset_collections()
            
            # Reinitialize with default knowledge
            await self.initialize_system()
            
            return "enhanced_ai_system_reset"
            
        except Exception as e:
            log.error(f"Error resetting system: {e}")
            return f"error: {str(e)}"

# Global enhanced AI system instance
_enhanced_ai_system = None

def get_enhanced_ai_system(api_key: Optional[str] = None) -> EnhancedAISystem:
    """Get the global enhanced AI system instance."""
    global _enhanced_ai_system
    if _enhanced_ai_system is None:
        _enhanced_ai_system = EnhancedAISystem(api_key)
    return _enhanced_ai_system

async def test_enhanced_ai_system():
    """Test the enhanced AI system functionality."""
    print("ü¶ñ Testing Enhanced AI System...")
    
    # Initialize enhanced AI system
    ai_system = get_enhanced_ai_system()
    
    # Initialize system
    print("üîß Initializing system...")
    result = await ai_system.initialize_system()
    print(f"Initialization result: {result}")
    
    # Test conversation handling
    print("üí¨ Testing conversation handling...")
    response = await ai_system.handle_conversation(
        "How do I test API authentication?"
    )
    print(f"Response: {response[:100]}...")
    
    # Test intelligent test generation
    print("üß™ Testing intelligent test generation...")
    tests = await ai_system.generate_intelligent_tests(
        "https://api.example.com",
        "Test user authentication endpoints"
    )
    print(f"Generated tests: {tests[:100]}...")
    
    # Test API specification analysis
    print("üìä Testing API specification analysis...")
    analysis = await ai_system.analyze_api_specification(
        "GET /users - Retrieve user list\nPOST /users - Create new user"
    )
    print(f"Analysis: {analysis[:100]}...")
    
    # Test troubleshooting
    print("üîß Testing troubleshooting...")
    troubleshooting = await ai_system.generate_troubleshooting_guide(
        "API returning 500 Internal Server Error"
    )
    print(f"Troubleshooting: {troubleshooting[:100]}...")
    
    # Get system stats
    print("üìä Getting system stats...")
    stats = await ai_system.get_system_stats()
    print(f"System Stats: {stats}")
    
    print("‚úÖ Enhanced AI system test completed!")

def main():
    """Main function for testing the enhanced AI system."""
    asyncio.run(test_enhanced_ai_system())

if __name__ == "__main__":
    main() 